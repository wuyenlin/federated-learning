{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Federated_Learning.ipynb","provenance":[{"file_id":"1sWdbt_a3Dya9TQKTB2k5p-kRJWiznGsb","timestamp":1587395601662}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JNBKOILTIJDs","colab_type":"text"},"source":["# Communication-Efficient Learning of Deep Networks from Decentralized Data"]},{"cell_type":"markdown","metadata":{"id":"5zmmGuEPIiF-","colab_type":"text"},"source":["## Helper files"]},{"cell_type":"code","metadata":{"id":"eHxXIQLJAFzo","colab_type":"code","colab":{}},"source":["# import global dependencies\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import copy\n","import numpy as np\n","from torchvision import datasets, transforms\n","import torch\n","import random\n","from torch import nn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nlFXcKdIBYBA","colab_type":"code","colab":{}},"source":["from torch import autograd\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn import metrics\n","\n","\n","class DatasetSplit(Dataset):\n","    def __init__(self, dataset, idxs):\n","        self.dataset = dataset\n","        self.idxs = list(idxs)\n","\n","    def __len__(self):\n","        return len(self.idxs)\n","\n","    def __getitem__(self, item):\n","        image, label = self.dataset[self.idxs[item]]\n","        return image, label\n","\n","\n","class LocalUpdate(object):\n","    def __init__(self, args, dataset=None, idxs=None):\n","        self.args = args\n","        self.loss_func = nn.CrossEntropyLoss()\n","        self.selected_clients = []\n","        self.ldr_train = DataLoader(DatasetSplit(dataset, idxs), batch_size=self.args.local_bs, shuffle=True)\n","\n","    def train(self, net):\n","        net.train()\n","        # train and update\n","        optimizer = torch.optim.SGD(net.parameters(), lr=self.args.lr, momentum=0.5)\n","\n","        epoch_loss = []\n","        for iter in range(self.args.local_ep):\n","            batch_loss = []\n","            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n","                images, labels = images.to(self.args.device), labels.to(self.args.device)\n","                net.zero_grad()\n","                log_probs = net(images)\n","                loss = self.loss_func(log_probs, labels)\n","                loss.backward()\n","                optimizer.step()\n","                if self.args.verbose and batch_idx % 10 == 0:\n","                    print('Update Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                        iter, batch_idx * len(images), len(self.ldr_train.dataset),\n","                               100. * batch_idx / len(self.ldr_train), loss.item()))\n","                batch_loss.append(loss.item())\n","            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n","        return net.state_dict(), sum(epoch_loss) / len(epoch_loss)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jL_C_dGoB0ch","colab_type":"code","colab":{}},"source":["class CNNMnist(nn.Module):\n","    def __init__(self, args):\n","        super(CNNMnist, self).__init__()\n","        self.conv1 = nn.Conv2d(args.num_channels, 10, kernel_size=5)\n","        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n","        self.conv2_drop = nn.Dropout2d()\n","        self.fc1 = nn.Linear(320, 50)\n","        self.fc2 = nn.Linear(50, args.num_classes)\n","\n","    def forward(self, x):\n","        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n","        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n","        x = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3])\n","        x = F.relu(self.fc1(x))\n","        x = F.dropout(x, training=self.training)\n","        x = self.fc2(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IYwXOWaUCD0M","colab_type":"code","colab":{}},"source":["import copy\n","\n","def FedAvg(w, clients):\n","    w_avg = copy.deepcopy(w[0])\n","    for k in w_avg.keys():\n","        for i in range(1, len(w)):\n","            tens = torch.mul(w[i][k], clients[i])\n","            w_avg[k] += tens\n","        w_avg[k] = torch.div(w_avg[k], sum(clients))\n","    return w_avg"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OOpNFnCQCSBu","colab_type":"code","colab":{}},"source":["import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","\n","\n","def test_img(net_g, datatest, args):\n","    net_g.eval()\n","    # testing\n","    test_loss = 0\n","    correct = 0\n","    data_loader = DataLoader(datatest, batch_size=args.bs)\n","    l = len(data_loader)\n","    for idx, (data, target) in enumerate(data_loader):\n","        if args.gpu != -1:\n","            data, target = data.cuda(), target.cuda()\n","        log_probs = net_g(data)\n","        # sum up batch loss\n","        test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()\n","        # get the index of the max log-probability\n","        y_pred = log_probs.data.max(1, keepdim=True)[1]\n","        correct += y_pred.eq(target.data.view_as(y_pred)).long().cpu().sum()\n","\n","    test_loss /= len(data_loader.dataset)\n","    accuracy = 100.00 * correct / len(data_loader.dataset)\n","    if args.verbose:\n","        print('\\nTest set: Average loss: {:.4f} \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n","            test_loss, correct, len(data_loader.dataset), accuracy))\n","    return accuracy, test_loss\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"njAouhS1Ivj7","colab_type":"text"},"source":["## Reproduction"]},{"cell_type":"code","metadata":{"id":"rLnBHMzN_KAn","colab_type":"code","colab":{}},"source":["# parse args\n","class args:\n","    gpu = 1 # <- -1 if no GPU is available\n","    device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n","    num_channels = 1\n","    num_users = 100\n","    num_classes = 10\n","    frac = 0.1\n","    lr = 0.1\n","    verbose = 0\n","    bs = 128\n","    epochs = 100\n","    \n","    iid = True        # < -This Value needs to be changed\n","    local_ep = 20     # <- This Value needs to be changed\n","    local_bs = 10     # <- This Value needs to be changed\n","\n","trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n","dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n","dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n","# sample users\n","if args.iid:\n","    dict_users = mnist_iid(dataset_train, args.num_users)\n","else:\n","    dict_users = mnist_noniid(dataset_train, args.num_users)\n","\n","img_size = dataset_train[0][0].shape\n","\n","# build model\n","\n","net_glob = CNNMnist(args=args).to(args.device)\n","print(net_glob)\n","net_glob.train()\n","\n","# copy weights\n","w_glob = net_glob.state_dict()\n","\n","# training\n","loss_train = []\n","cv_loss, cv_acc = [], []\n","val_loss_pre, counter = 0, 0\n","net_best = None\n","best_loss = None\n","val_acc_list, net_list = [], []"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cC7esCT0MJKQ","colab_type":"code","colab":{}},"source":["for iter in range(args.epochs):\n","\n","    w_locals, loss_locals = [], []\n","    m = max(int(args.frac * args.num_users), 1)\n","    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n","    for idx in idxs_users:\n","        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n","        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n","        w_locals.append(copy.deepcopy(w))\n","        loss_locals.append(copy.deepcopy(loss))\n","    # update global weights\n","    w_glob = FedAvg(w_locals)\n","\n","    # copy weight to net_glob\n","    net_glob.load_state_dict(w_glob)\n","\n","    # print loss\n","    loss_avg = sum(loss_locals) / len(loss_locals)\n","    \n","    loss_train.append(loss_avg)\n","    \n","    # Evaluate score\n","    net_glob.eval()\n","    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n","    print('Round {:3d}, Average loss {:.3f}, Accuracy {:.3f}'.format(iter, loss_avg, acc_test))\n","\n","# testing\n","net_glob.eval()\n","acc_train, loss_train = test_img(net_glob, dataset_train, args)\n","acc_test, loss_test = test_img(net_glob, dataset_test, args)\n","print(\"Training accuracy: {:.2f}\".format(acc_train))\n","print(\"Testing accuracy: {:.2f}\".format(acc_test))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C0uGPKP3I9dl","colab_type":"text"},"source":["## Uneven Distribution"]},{"cell_type":"code","metadata":{"id":"BcJaPRU1KTq6","colab_type":"code","colab":{}},"source":["def mnist_iid(dataset, num_users):\n","    \"\"\"\n","    Sample I.I.D. client data from MNIST dataset\n","    :param dataset:\n","    :param num_users:\n","    :return: dict of image index\n","    \"\"\"\n","    num_items = int(len(dataset)/num_users)\n","    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n","    for i in range(num_users):\n","        dict_users[i] = set(np.random.choice(\n","            all_idxs,\n","            random.randint(1,num_items),\n","            replace=False))\n","        print(len(dict_users[i]))\n","        all_idxs = list(set(all_idxs) - dict_users[i])\n","    return dict_users"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LwHcoyrVLTHU","colab_type":"code","colab":{}},"source":["for iter in range(args.epochs):\n","\n","    w_locals, loss_locals = [], []\n","    m = max(int(args.frac * args.num_users), 1)\n","    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n","    for idx in idxs_users:\n","        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n","        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n","        w_locals.append(copy.deepcopy(w))\n","        loss_locals.append(copy.deepcopy(loss))\n","    # update global weights\n","    w_glob = FedAvg(w_locals)\n","\n","    # copy weight to net_glob\n","    net_glob.load_state_dict(w_glob)\n","\n","    # print loss\n","    loss_avg = sum(loss_locals) / len(loss_locals)\n","    \n","    loss_train.append(loss_avg)\n","    \n","    # Evaluate score\n","    net_glob.eval()\n","    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n","    print('Round {:3d}, Average loss {:.3f}, Accuracy {:.3f}'.format(iter, loss_avg, acc_test))\n","\n","# testing\n","net_glob.eval()\n","acc_train, loss_train = test_img(net_glob, dataset_train, args)\n","acc_test, loss_test = test_img(net_glob, dataset_test, args)\n","print(\"Training accuracy: {:.2f}\".format(acc_train))\n","print(\"Testing accuracy: {:.2f}\".format(acc_test))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jsr9RS1cJDC4","colab_type":"text"},"source":["## Uneven Distribution with Weights"]},{"cell_type":"code","metadata":{"id":"Su58lzAmLngv","colab_type":"code","colab":{}},"source":["def FedAvg(w, clients):\n","    w_avg = copy.deepcopy(w[0])\n","    for k in w_avg.keys():\n","        for i in range(1, len(w)):\n","            tens = torch.mul(w[i][k], clients[i])\n","            w_avg[k] += tens\n","        w_avg[k] = torch.div(w_avg[k], sum(clients))\n","    return w_avg"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GrkWmj6zAXga","colab_type":"code","colab":{}},"source":["def mnist_iid(dataset, num_users):\n","    \"\"\"\n","    Sample I.I.D. client data from MNIST dataset\n","    :param dataset:\n","    :param num_users:\n","    :return: dict of image index\n","    \"\"\"\n","    num_items = int(len(dataset)/num_users)\n","    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n","    for i in range(num_users):\n","        dict_users[i] = set(np.random.choice(\n","            all_idxs,\n","            random.randint(1,num_items),\n","            replace=False))\n","        print(len(dict_users[i]))\n","        all_idxs = list(set(all_idxs) - dict_users[i])\n","    return dict_users"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4kiwPkSZ_4Ba","colab_type":"code","colab":{}},"source":["for iter in range(args.epochs):\n","\n","    w_locals, loss_locals, num_items = [], [], []\n","    m = max(int(args.frac * args.num_users), 1)\n","    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n","    for idx in idxs_users:\n","        num_items.append(len(dict_users[idx]))\n","        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n","        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n","        w_locals.append(copy.deepcopy(w))\n","        loss_locals.append(copy.deepcopy(loss))\n","    # update global weights\n","    w_glob = FedAvg(w_locals, num_items)\n","\n","    # copy weight to net_glob\n","    net_glob.load_state_dict(w_glob)\n","\n","    # print loss\n","    loss_avg = sum(loss_locals) / len(loss_locals)\n","    \n","    loss_train.append(loss_avg)\n","    \n","    # Evaluate score\n","    net_glob.eval()\n","    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n","    print('Round {:3d}, Average loss {:.3f}, Accuracy {:.3f}'.format(iter, loss_avg, acc_test))\n","\n","# testing\n","net_glob.eval()\n","acc_train, loss_train = test_img(net_glob, dataset_train, args)\n","acc_test, loss_test = test_img(net_glob, dataset_test, args)\n","print(\"Training accuracy: {:.2f}\".format(acc_train))\n","print(\"Testing accuracy: {:.2f}\".format(acc_test))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5E19mxNYJRcN","colab_type":"text"},"source":["## Adding Noise"]},{"cell_type":"code","metadata":{"id":"oC9ND0K_YU5x","colab_type":"code","colab":{}},"source":["for iter in range(args.epochs):\n","\n","    w_locals, loss_locals = [], []\n","    m = max(int(args.frac * args.num_users), 1)\n","    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n","    for idx in idxs_users:\n","        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n","        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n","        \n","        # Add noise to layers\n","        sigma_squared = 0.3\n","        for layer in w:\n","            x = np.random.normal(0,sigma_squared,w[layer].size())\n","            x = np.reshape(x,w[layer].size())\n","            x = torch.from_numpy(x)\n","            w[layer] = w[layer]+x.cuda()\n","\n","        w_locals.append(copy.deepcopy(w))\n","        loss_locals.append(copy.deepcopy(loss))\n","    # update global weights\n","    w_glob = FedAvg(w_locals)\n","\n","    # copy weight to net_glob\n","    net_glob.load_state_dict(w_glob)\n","\n","    # print loss\n","    loss_avg = sum(loss_locals) / len(loss_locals)\n","    \n","    loss_train.append(loss_avg)\n","    \n","    # Evaluate score\n","    net_glob.eval()\n","    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n","    print('Round {:3d}, Average loss {:.3f}, Accuracy {:.3f}'.format(iter, loss_avg, acc_test))\n","\n","# testing\n","net_glob.eval()\n","acc_train, loss_train = test_img(net_glob, dataset_train, args)\n","acc_test, loss_test = test_img(net_glob, dataset_test, args)\n","print(\"Training accuracy: {:.2f}\".format(acc_train))\n","print(\"Testing accuracy: {:.2f}\".format(acc_test))"],"execution_count":0,"outputs":[]}]}